# hello-crew-tools: LLM Tool Capability Validation

## ğŸš€ Quick Start

### Setup

```bash
# Copy .env.example to .env and add your OpenAI API key
cp .env.example .env
# Edit .env and paste your API key from https://platform.openai.com/api-keys

# Build and run
make build
make run
```

### Test Tool Execution

```bash
./test_time.sh
```

---

## ğŸ¯ Purpose

This example validates whether the memory problem in `hello-crew` is due to:
1. **Architecture limitation** (no persistence) - OR
2. **LLM limitation** (model ignores memory instructions)

By providing tools for conversation analysis, we test if the LLM can use structured data to accurately count messages and remember facts.

---

## ğŸ“Š Problem Context

From `hello-crew` testing, we observed:

```
Query 1: "TÃ´i tÃªn gÃ¬?"
â†’ "Phan Minh TÃ i" âœ“ Correct

Query 2: "TÃ´i lÃ  LÃª VÄƒn PhÆ°Æ¡ng Trang"
â†’ Updated name âœ“

Query 3: "TÃ´i tÃªn gÃ¬?"
â†’ "LÃª VÄƒn PhÆ°Æ¡ng Trang" âœ“ Remembers update

Query 4: "TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?"
â†’ "start fresh" âœ— CANNOT COUNT
```

**Why Query 4 failed:**
- Without tools: Agent must parse raw conversation text to count = unreliable
- With tools: Agent calls GetMessageCount() function = accurate

---

## ğŸ› ï¸ Available Tools

### 1. `get_message_count()`
**Purpose:** Returns total number of messages in conversation

**Usage:**
```
User: "TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?"
â†’ Agent calls: get_message_count()
â†’ Tool returns: {"count": 4, "role_breakdown": {"user": 2, "assistant": 2}}
â†’ Agent responds: "Báº¡n Ä‘Ã£ há»i 2 cÃ¢u"
```

**Expected Improvement:**
- Before: "TÃ´i khÃ´ng biáº¿t" âŒ
- After: "Báº¡n Ä‘Ã£ há»i 2 cÃ¢u" âœ…

---

### 2. `get_conversation_summary()`
**Purpose:** Returns all messages and extracted facts

**Usage:**
```
User: "TÃ´i lÃ  ai?"
â†’ Agent calls: get_conversation_summary()
â†’ Tool returns: {
    "total_messages": 4,
    "messages": [...],
    "extracted_facts": {"user_name": "LÃª VÄƒn PhÆ°Æ¡ng Trang", ...}
}
â†’ Agent responds: "Báº¡n lÃ  LÃª VÄƒn PhÆ°Æ¡ng Trang"
```

---

### 3. `search_messages(query, limit)`
**Purpose:** Search for keywords in conversation

**Usage:**
```
User: "Báº¡n nhá»› tÃ´i nÃ³i gÃ¬ láº§n Ä‘áº§u?"
â†’ Agent calls: search_messages(query="tÃªn")
â†’ Tool returns: results with matching messages
â†’ Agent responds: "Báº¡n há»i 'TÃ´i tÃªn gÃ¬?'"
```

---

### 4. `count_messages_by(filter_by, filter_value)`
**Purpose:** Count messages filtered by role or keyword

**Usage:**
```
User: "TÃ´i Ä‘Ã£ nÃ³i bao nhiÃªu láº§n?"
â†’ Agent calls: count_messages_by(filter_by="role", filter_value="user")
â†’ Tool returns: {"count": 2}
â†’ Agent responds: "Báº¡n Ä‘Ã£ nÃ³i 2 láº§n"
```

---

## ğŸš€ Quick Start

### Prerequisites
- Go 1.20+
- Ollama running locally with `deepseek-r1:1.5b` or `gemma3:1b`

### Step 1: Start Ollama
```bash
ollama run deepseek-r1:1.5b
```

### Step 2: Run the Example
```bash
cd examples/00-hello-crew-tools
make run
```

### Step 3: Test Conversations

Try these conversations in order:

**Conversation 1: Basic Greeting**
```
> Xin chÃ o!
Agent: Hello! How can I help you today?

> TÃ´i lÃ  John Doe
Agent: Got it, your name is John Doe! ğŸ˜Š
```

**Conversation 2: Memory Test (Name Recall)**
```
> TÃ´i tÃªn gÃ¬?
Agent: Your name is John Doe

Expected: âœ… Agent remembers the name
```

**Conversation 3: Tool Test (Message Counting)**
```
> TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?
Expected: Agent calls get_message_count()
Expected Result:
  - With tools working: "Báº¡n Ä‘Ã£ há»i 3 cÃ¢u"  âœ…
  - If tools fail: "I don't know" or random answer âŒ
```

**Conversation 4: Tool Test (Message Search)**
```
> Báº¡n nhá»› tÃ´i nÃ³i gÃ¬ láº§n Ä‘áº§u?
Expected: Agent calls search_messages()
Expected Result: "Báº¡n nÃ³i 'Xin chÃ o!'"
```

**Conversation 5: Tool Test (Fact Extraction)**
```
> Báº¡n biáº¿t tÃ´i lÃ  ai?
Expected: Agent calls get_conversation_summary()
Expected Result: "Báº¡n lÃ  John Doe"
```

---

## ğŸ“ˆ Success Criteria

### If Tools Improve Memory âœ…
```
Metric                  | Before    | After
------------------------|-----------|----------
Can count messages      | NO âŒ     | YES âœ…
Can recall facts        | NO âŒ     | YES âœ…
Can search history      | NO âŒ     | YES âœ…
Overall accuracy        | 0%        | 95%+
```

**Conclusion:** Memory problem = **Architecture** (lack of structure)
**Next Step:** Implement Simple Path with persistence

---

### If Tools Don't Help âŒ
```
Metric                  | Expected  | Actual
------------------------|-----------|----------
Agent calls tools       | 100%      | < 50%
Tool results used       | 100%      | < 20%
Answer accuracy         | 95%+      | < 30%
Model follows prompts   | YES       | NO
```

**Conclusion:** Memory problem = **LLM limitation** (model can't follow instructions)
**Next Step:** Switch to better models (Claude, GPT-4) or use different strategy

---

## ğŸ” What's Different from hello-crew?

| Aspect | hello-crew | hello-crew-tools |
|--------|-----------|------------------|
| **Tools** | None | 4 conversation analysis tools |
| **Data Access** | Raw history only | Structured tool results |
| **Message Counting** | No | Yes (via tool) |
| **Fact Extraction** | No | Yes (via tool) |
| **Search Capability** | No | Yes (via tool) |
| **Expected Accuracy** | Low | High (if tools work) |

---

## ğŸ§ª Testing Scenarios

### Scenario A: Tools Are Effective
```
Conversation Flow:
1. "TÃ´i tÃªn gÃ¬?" â†’ Agent stores name
2. "TÃ´i lÃ  John" â†’ Agent updates name
3. "TÃ´i tÃªn gÃ¬?" â†’ Agent recalls "John" âœ“
4. "TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?" â†’ Agent uses get_message_count() tool â†’ "3" âœ“
5. "Báº¡n nhá»› láº§n Ä‘áº§u tÃ´i nÃ³i gÃ¬?" â†’ Agent uses search_messages() â†’ "TÃ´i tÃªn gÃ¬?" âœ“

Observation: Tools enable accurate conversation understanding
```

### Scenario B: Tools Don't Work
```
Conversation Flow:
1-3. Same as above (name works)
4. "TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?" â†’ Agent ignores tool â†’ "I don't know" âœ—
5. "Báº¡n nhá»› láº§n Ä‘áº§u tÃ´i nÃ³i gÃ¬?" â†’ Agent guesses â†’ Random answer âœ—

Observation: LLM doesn't call or use tools properly
```

---

## ğŸ“Š Logging & Debugging

The example logs tool execution details:

```
[TOOL CALL] Tool: get_message_count
[TOOL RESULT] {"count": 4, "role_breakdown": {"user": 2, "assistant": 2}}
[AGENT RESPONSE] Based on that data...
```

You can see:
- Which tools the agent called
- What data was returned
- How the agent used the results

---

## ğŸ”§ Server Mode (API Testing)

Run in server mode for API testing:

```bash
make server
```

This starts the server on `http://localhost:8082`

### Endpoints

**POST /execute** - Execute with agent
```bash
curl -X POST http://localhost:8082/execute \
  -H "Content-Type: application/json" \
  -d '{"input": "TÃ´i Ä‘Ã£ há»i máº¥y cÃ¢u?"}'
```

**POST /execute-tool** - Execute a specific tool directly
```bash
curl -X POST http://localhost:8082/execute-tool \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "get_message_count",
    "params": {}
  }'
```

**GET /tools** - List available tools
```bash
curl http://localhost:8082/tools
```

**GET /health** - Health check
```bash
curl http://localhost:8082/health
```

---

## ğŸ“ What This Teaches Us

### If Tools Work (Likely Outcome)
1. **LLM can use tools** when properly configured
2. **Architecture matters** - structured data > raw text
3. **Next step:** Implement persistent storage + tool-based retrieval
4. **Simple Path is feasible** - tools can provide structure

### If Tools Don't Work (Unlikely)
1. **Model limitation** - Ollama is too weak
2. **System prompt issue** - Tool instructions aren't clear enough
3. **API issue** - Tool calling mechanism isn't working
4. **Next step:** Debug with Claude/GPT-4 or fix tool definition

---

## ğŸš¦ Troubleshooting

### Agent doesn't call tools
**Check:**
1. Tool definitions in `hello-agent-tools.yaml` are valid
2. System prompt includes tool usage instructions
3. Ollama model supports function calling (deepseek-r1, gemma3 should work)

### Tools return empty results
**Check:**
1. Conversation history is being accumulated
2. Tool parameters are correctly passed
3. Message content is not empty

### Ollama connection fails
**Check:**
1. Ollama is running: `ollama serve`
2. Model is downloaded: `ollama pull deepseek-r1:1.5b`
3. Local port is 11434 (default)

---

## ğŸ“ Files Structure

```
hello-crew-tools/
â”œâ”€â”€ DESIGN.md                              # Detailed design document
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ Makefile                               # Build commands
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                            # Entry point with tool integration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ crew.yaml                          # Crew configuration
â”‚   â””â”€â”€ agents/
â”‚       â””â”€â”€ hello-agent-tools.yaml         # Agent with tool definitions
â”œâ”€â”€ internal/
â”‚   â””â”€â”€ tools.go                           # Tool implementations
â”œâ”€â”€ go.mod
â””â”€â”€ go.sum
```

---

## ğŸ¯ Key Takeaways

1. **This example validates a hypothesis** about the root cause of memory problems
2. **Tools provide structure** that raw history lacks
3. **The outcome will guide** the architecture design for Simple Path
4. **Results will inform** whether to focus on persistence vs. LLM upgrade

---

## ğŸ“š Related Examples

- `hello-crew` - Original example with memory issues
- `hello-crew-persistence` (coming) - File-based session storage
- `hello-crew-semantic` (coming) - Vector embeddings + semantic search

---

## ğŸ¤ Contributing

To add more tools:

1. Add tool definition to `hello-agent-tools.yaml`
2. Implement tool method in `internal/tools.go`
3. Add test case
4. Update this README

---

## ğŸ“ Questions?

Check DESIGN.md for architectural details and implementation notes.
