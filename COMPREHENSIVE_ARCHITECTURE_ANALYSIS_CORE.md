# ğŸ“Š PhÃ¢n TÃ­ch Kiáº¿n TrÃºc ToÃ n Diá»‡n ThÆ° Má»¥c `./core`

## Tá»•ng Quan Executive

ThÆ° má»¥c `./core` lÃ  **ná»n táº£ng kiáº¿n trÃºc lÃµi** cá»§a há»‡ thá»‘ng multi-agents, chá»©a **9,436 dÃ²ng mÃ£ Go** Ä‘Æ°á»£c tá»• chá»©c thÃ nh **20 file** (13 file chÃ­nh + 7 file test). ÄÃ¢y lÃ  má»™t **thÆ° viá»‡n production-ready** Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c á»©ng dá»¥ng Ä‘a-agent phÃ¢n tÃ¡n vá»›i **kháº£ nÄƒng resilience cao**, **monitoring toÃ n diá»‡n**, vÃ  **cÆ¡ cháº¿ Ä‘iá»u phá»‘i phá»©c táº¡p**.

---

## 1. ğŸ—ï¸ KIáº¾N TRÃšC Cáº¤P CAO

### 1.1 Tá»•ng Thá»ƒ Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HTTP Server Layer                       â”‚
â”‚         (http.go) - SSE Streaming, REST Handlers           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Crew Execution Engine                          â”‚
â”‚  (crew.go) - Orchestration, Routing, Handoff, Parallel   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Agent Layer  â”‚    â”‚ Tool Execution â”‚
         â”‚ (agent.go)   â”‚    â”‚ (crew.go)      â”‚
         â”‚              â”‚    â”‚                â”‚
         â”‚ â€¢ Execute    â”‚    â”‚ â€¢ Safe Wrapper â”‚
         â”‚ â€¢ Tool Call  â”‚    â”‚ â€¢ Retry Logic  â”‚
         â”‚ â€¢ Response   â”‚    â”‚ â€¢ Timeout Mgmt â”‚
         â”‚   Parse      â”‚    â”‚ â€¢ Error Class. â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Configuration & Validation Layer    â”‚
         â”‚  (config.go, validation.go, http.go)  â”‚
         â”‚                                        â”‚
         â”‚  â€¢ YAML Loading & Parsing             â”‚
         â”‚  â€¢ Circular Routing Detection         â”‚
         â”‚  â€¢ Input Validation                   â”‚
         â”‚  â€¢ Signal Matching                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Monitoring & Observability             â”‚
         â”‚  (metrics.go, request_tracking.go)      â”‚
         â”‚                                         â”‚
         â”‚  â€¢ Metrics Collection                  â”‚
         â”‚  â€¢ Request ID Tracking                 â”‚
         â”‚  â€¢ Performance Monitoring              â”‚
         â”‚  â€¢ Error Classification                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 PhÃ¢n Bá»‘ TrÃ¡ch Nhiá»‡m

| Layer | File(s) | TrÃ¡ch Nhiá»‡m |
|-------|---------|-------------|
| **HTTP/Network** | `http.go`, `streaming.go` | Giao tiáº¿p SSE, input validation, response formatting |
| **Orchestration** | `crew.go` | Äiá»u phá»‘i agent, routing, handoff, parallel execution |
| **Agent Execution** | `agent.go` | Gá»i OpenAI API, tool call extraction, prompt building |
| **Configuration** | `config.go`, `validation.go` | Load YAML, validate, detect circular routing |
| **Tool Execution** | `crew.go` (executeCalls) | Wrapper an toÃ n, retry logic, timeout management |
| **Monitoring** | `metrics.go`, `request_tracking.go` | Metrics collection, request correlation, logging |
| **Lifecycle** | `shutdown.go` | Graceful shutdown, cleanup |
| **Testing** | `*_test.go` (7 files) | Unit/integration tests |

---

## 2. ğŸ“‹ CHI TIáº¾T Tá»ªNG COMPONENT

### 2.1 Core Data Structures (types.go)

**Má»¥c Ä‘Ã­ch**: Äá»‹nh nghÄ©a cÃ¡c kiá»ƒu dá»¯ liá»‡u ná»n táº£ng cá»§a há»‡ thá»‘ng

```go
// Agent - ÄÆ¡n vá»‹ thá»±c thi Ä‘á»™c láº­p
Agent {
  ID              string        // Äá»‹nh danh duy nháº¥t
  Name            string        // TÃªn hiá»ƒn thá»‹
  Role            string        // Vai trÃ² (Analyst, Decision Maker, etc.)
  Backstory       string        // Ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p cho LLM
  Model           string        // Model LLM (gpt-4o-mini, etc.)
  SystemPrompt    string        // Custom system prompt (tá»« config)
  Tools           []*Tool       // Danh sÃ¡ch cÃ´ng cá»¥ cÃ³ sáºµn
  Temperature     float64       // Creativity parameter
  IsTerminal      bool          // LÃ  agent cuá»‘i cÃ¹ng?
  HandoffTargets  []string      // Danh sÃ¡ch agent cÃ³ thá»ƒ chuyá»ƒn tá»›i
}

// Tool - HÃ m cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i bá»Ÿi agent
Tool {
  Name            string                              // TÃªn cÃ´ng cá»¥
  Description     string                              // MÃ´ táº£ chá»©c nÄƒng
  Parameters      map[string]interface{}              // Schema tham sá»‘
  Handler         func(ctx, args) (string, error)    // Callback thá»±c thi
}

// Crew - NhÃ³m agent lÃ m viá»‡c cÃ¹ng nhau
Crew {
  Agents          []*Agent      // Danh sÃ¡ch agent
  Tasks           []*Task       // Danh sÃ¡ch task
  MaxRounds       int           // Sá»‘ vÃ²ng tá»‘i Ä‘a
  MaxHandoffs     int           // Sá»‘ láº§n chuyá»ƒn tá»‘i Ä‘a
  Routing         *RoutingConfig // Cáº¥u hÃ¬nh Ä‘iá»u phá»‘i (tá»« crew.yaml)
}

// StreamEvent - Sá»± kiá»‡n Ä‘Æ°á»£c gá»­i qua SSE
StreamEvent {
  Type            string        // "agent_start", "agent_response", "tool_start", etc.
  Agent           string        // TÃªn/ID agent
  Content         string        // Ná»™i dung chÃ­nh
  Timestamp       time.Time     // Thá»i Ä‘iá»ƒm xáº£y ra
  Metadata        interface{}   // Dá»¯ liá»‡u bá»• sung
}
```

**Ã nghÄ©a kiáº¿n trÃºc**:
- PhÃ¢n tÃ¡ch rÃµ rÃ ng giá»¯a **Agent** (Ä‘iá»u phá»‘i logic), **Tool** (cÃ´ng cá»¥ gá»i), **Crew** (nhÃ³m)
- Há»— trá»£ **streaming events** cho real-time monitoring
- **SystemPrompt** cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¹y chá»‰nh per-agent thay vÃ¬ global

---

### 2.2 Agent Execution (agent.go - 469 dÃ²ng)

**Má»¥c Ä‘Ã­ch**: Gá»i LLM API, trÃ­ch xuáº¥t tool calls, xá»­ lÃ½ responses

#### Key Features:

**A. OpenAI Client Caching (Lines 16-85)**
```
PROBLEM (Issue #2): RÃ² rá»‰ bá»™ nhá»› - má»—i request táº¡o má»›i client
SOLUTION:
  âœ… Client cache vá»›i TTL expiration
  âœ… Sliding window: TTL lÃ m má»›i trÃªn má»—i access
  âœ… Cleanup goroutine má»—i 5 phÃºt (tá»± Ä‘á»™ng dá»n expired clients)

IMPLEMENTATION:
  â€¢ clientEntry: {client, createdAt, expiresAt}
  â€¢ clientTTL = 1 hour
  â€¢ init() starts cleanup goroutine
  â€¢ getOrCreateOpenAIClient: returns cached or creates new
```

**B. ExecuteAgent (Lines 87-156)**
```
Flow:
  1. Get or create OpenAI client (tá»« cache)
  2. Build system prompt (custom hoáº·c generic)
  3. Convert history to OpenAI message format
  4. Call OpenAI Chat Completions API
  5. Extract tool calls (primary + fallback methods)
  6. Return AgentResponse

Error Handling:
  â€¢ API errors caught and logged
  â€¢ Empty response detection
```

**C. Tool Call Extraction (Lines 120-156, 275-356, 358-424)**
```
PRIMARY METHOD (Preferred):
  extractFromOpenAIToolCalls()
  â€¢ Sá»­ dá»¥ng OpenAI's native tool_calls field
  â€¢ Validated bá»Ÿi OpenAI trÆ°á»›c khi tráº£ vá»
  â€¢ JSON argument parsing
  â€¢ Tool existence validation

FALLBACK METHOD (Legacy Support):
  extractToolCallsFromText()
  â€¢ PhÃ¢n tÃ­ch text response cho pattern ToolName(...)
  â€¢ parseToolArguments() xá»­ lÃ½ nested brackets/quotes
  â€¢ getToolParameterNames() mapping positional â†’ named args
  â€¢ Pattern matching cho tool names

RATIONALE:
  âœ… Hybrid approach handles cáº£:
     - Modern models vá»›i native tool_calls support
     - Legacy models hoáº·c edge cases
```

**D. System Prompt Building (Lines 158-203)**
```
Logic:
  1. Náº¿u agent cÃ³ SystemPrompt tÃ¹y chá»‰nh â†’ dÃ¹ng nÃ³
     (template variables: {{name}}, {{role}}, {{backstory}})
  2. Náº¿u khÃ´ng â†’ build generic prompt:
     - Agent name/role/backstory
     - Tool list (náº¿u cÃ³)
     - Tool call instructions (ToolName(param1, param2))
     - Instructions for analysis & tool usage
     - Terminal agent indicator (náº¿u applicable)
```

**Architectural Insights**:
- âœ… **Stateless**: ExecuteAgent khÃ´ng giá»¯ state (idempotent)
- âœ… **Client reuse**: Caching prevents connection leaks
- âœ… **Dual-mode tool extraction**: Flexibility for different model types
- âœ… **Prompt customization**: Per-agent system prompts tá»« YAML config

---

### 2.3 Crew Execution Engine (crew.go - 1,437 dÃ²ng)

**Má»¥c Ä‘Ã­ch**: Äiá»u phá»‘i multiple agents, routing, timeout management, parallel execution

#### 2.3.1 Core Execution Loop (ExecuteStream / Execute)

**Non-Streaming Mode (Execute)**:
```
Entry Point: ExecuteStream(ctx, input, streamChan)
â”œâ”€ Determine starting agent (resume agent or entry agent)
â”œâ”€ MAIN LOOP: for each agent
â”‚  â”œâ”€ [1] Execute agent (call LLM)
â”‚  â”‚   â””â”€ Record metrics (duration, success/failure)
â”‚  â”œâ”€ [2] Send agent response event to stream
â”‚  â”œâ”€ [3] Add response to history
â”‚  â”œâ”€ [4] TOOL EXECUTION PHASE (if tool calls exist)
â”‚  â”‚   â”œâ”€ Execute each tool (with timeout tracking)
â”‚  â”‚   â”œâ”€ Add results to history
â”‚  â”‚   â””â”€ Feed back to agent for analysis
â”‚  â”‚   â””â”€ Loop back to [1] (agent processes results)
â”‚  â”œâ”€ [5] CHECK ROUTING (config-driven)
â”‚  â”‚   â””â”€ If agent emits routing signal â†’ handoff to target agent
â”‚  â”œâ”€ [6] CHECK WAIT_FOR_SIGNAL (pause mechanism)
â”‚  â”‚   â””â”€ If enabled â†’ emit PAUSE event and return
â”‚  â”œâ”€ [7] TERMINAL CHECK
â”‚  â”‚   â””â”€ If IsTerminal â†’ return (end execution)
â”‚  â”œâ”€ [8] PARALLEL GROUP CHECK
â”‚  â”‚   â””â”€ If signal matches parallel group â†’ execute parallel agents
â”‚  â””â”€ [9] HANDOFF (normal flow)
â”‚     â””â”€ Find next agent and continue
â””â”€ RETURN when:
   â€¢ Terminal agent reached, OR
   â€¢ Max handoffs exceeded, OR
   â€¢ wait_for_signal triggered, OR
   â€¢ No more agents available
```

**Key Control Flow Elements**:

```
1. TOOL EXECUTION LOOP
   for each ToolCall:
     â”œâ”€ Validate tool exists
     â”œâ”€ Check sequence deadline
     â”œâ”€ Calculate per-tool timeout (remaining sequence time)
     â”œâ”€ Execute with context timeout
     â”œâ”€ Record metrics
     â””â”€ Check timeout warning threshold

2. ROUTING SIGNAL MATCHING
   Lines 1063-1095: signalMatchesContent()
   â”œâ”€ Exact match: "signal" in content
   â”œâ”€ Normalized match: after TrimSpace
   â””â”€ Bracket variations: "[signal]" matches "[ signal ]"
      (handles Vietnamese text with spacing)

3. WAIT_FOR_SIGNAL (Pause Mechanism)
   Lines 609-616 (Stream), 755-767 (Non-Stream)
   â”œâ”€ Check AgentBehavior.WaitForSignal in config
   â”œâ”€ If true â†’ emit PAUSE event with agent ID
   â””â”€ Execution returns, waiting for user input

4. PARALLEL EXECUTION
   Lines 623-667 (Stream), 780-822 (Non-Stream)
   â”œâ”€ Find ParallelGroup from routing config
   â”œâ”€ Launch all agents in parallel (goroutines/errgroup)
   â”œâ”€ CollectResults with mutual exclusion
   â””â”€ Aggregate results â†’ pass to next agent
```

#### 2.3.2 Timeout Management (Issue #11 & #4 Fixes)

**TimeoutTracker (Lines 285-359)**:
```go
type TimeoutTracker struct {
  sequenceStartTime time.Time     // Khi sequence báº¯t Ä‘áº§u
  sequenceDeadline  time.Time     // Deadline cá»§a cáº£ sequence
  overheadBudget    time.Duration // Reserved cho LLM calls (500ms default)
  usedTime          time.Duration // ÄÃ£ dÃ¹ng bao nhiÃªu
  mu                sync.Mutex    // Thread-safe
}

Key Methods:
â”œâ”€ GetRemainingTime()
â”‚  â””â”€ Returns max(0, time until deadline)
â”œâ”€ CalculateToolTimeout(defaultTimeout, perToolTimeout)
â”‚  â””â”€ Returns min(perToolTimeout, remaining - overhead)
â”œâ”€ RecordToolExecution(duration)
â”‚  â””â”€ Track cumulative used time
â””â”€ IsTimeoutWarning()
   â””â”€ true if within 20% of deadline
```

**Tool Execution with Timeout (Lines 1013-1050)**:
```
Flow:
  1. setupSequenceContext() â†’ creates timeout context for whole sequence
  2. For each tool:
     â”œâ”€ Check sequence deadline (fail fast if exceeded)
     â”œâ”€ Calculate per-tool timeout accounting for remaining time
     â”œâ”€ Create tool-specific context with calculated timeout
     â”œâ”€ Execute tool (with safeExecuteTool wrapper)
     â”œâ”€ Record execution in timeout tracker
     â”œâ”€ Detect timeout: errors.Is(err, context.DeadlineExceeded)
     â””â”€ Check warning threshold: IsTimeoutWarning()
  3. Return collected results
```

**Why This Matters**:
- âœ… **Sequential timeout**: Total time for all tools bounded
- âœ… **Per-tool timeout**: Individual tools can't consume all time
- âœ… **Remaining time tracking**: Smart allocation based on progress
- âœ… **Overhead budget**: Reserve time for LLM processing between tools

#### 2.3.3 Tool Execution with Error Recovery (Issue #5)

**safeExecuteTool (Lines 264-270, 189-270)**:
```go
// Main entry point - uses retry wrapper
safeExecuteTool(ctx, tool, args) â†’ retryWithBackoff(maxRetries: 2)

// Actual execution with panic recovery
safeExecuteToolOnce(ctx, tool, args):
  â”œâ”€ defer-recover() // Catch panic, convert to error
  â”œâ”€ validateToolArguments(tool, args)
  â””â”€ tool.Handler(ctx, args) // Execute

// Retry logic with exponential backoff
retryWithBackoff(ctx, tool, args, maxRetries: 2):
  â”œâ”€ For each attempt (0 to maxRetries):
  â”‚  â”œâ”€ Execute tool
  â”‚  â”œâ”€ If success â†’ return
  â”‚  â”œâ”€ Classify error type
  â”‚  â””â”€ If non-retryable â†’ fail immediately
  â”‚  â””â”€ If retryable â†’ wait exponential backoff + jitter
  â”‚  â””â”€ Check context not cancelled during backoff
  â””â”€ Return error after all retries exhausted

// Error Classification (classifyError)
â”œâ”€ ErrorTypeTimeout â†’ RETRYABLE (transient)
â”œâ”€ ErrorTypeNetwork â†’ RETRYABLE (transient)
â”œâ”€ ErrorTypeTemporary â†’ RETRYABLE (transient)
â”œâ”€ ErrorTypePanic â†’ NON-RETRYABLE (permanent)
â”œâ”€ ErrorTypeValidation â†’ NON-RETRYABLE (permanent)
â””â”€ ErrorTypePermanent â†’ NON-RETRYABLE (permanent)

// Backoff calculation (calculateBackoffDuration)
Duration = min(100ms * 2^attempt, 5s)
// Example: 100ms, 200ms, 400ms, 800ms, (capped at 5s)
```

**Architecture Benefits**:
- âœ… **Transient vs permanent failures**: Smart retry decisions
- âœ… **Exponential backoff**: Prevents thundering herd
- âœ… **Panic recovery**: One tool can't crash system
- âœ… **Limited retries**: Max 2 retries prevents infinite loops

#### 2.3.4 Metrics Collection (Issue #14)

**Per-Execution Recording**:
```
Agent Execution:
  RecordAgentExecution(agentID, agentName, duration, success)

Tool Execution:
  RecordToolExecution(toolName, duration, success)

Metrics Flow:
  1. Agent runs â†’ track duration, record in metrics
  2. Tool runs â†’ track duration, record in metrics
  3. Timeout detected â†’ record as timeout event
  4. Error occurs â†’ record failure + error type
```

**MetricsCollector (metrics.go)**:
```
SystemMetrics:
  â”œâ”€ TotalRequests, SuccessfulRequests, FailedRequests
  â”œâ”€ TotalExecutionTime, AverageRequestTime
  â”œâ”€ AgentMetrics map[agentID]*AgentMetrics
  â”‚  â”œâ”€ ExecutionCount, SuccessCount, ErrorCount, TimeoutCount
  â”‚  â”œâ”€ TotalDuration, AverageDuration, MinDuration, MaxDuration
  â”‚  â””â”€ ToolMetrics map[toolName]*ToolMetrics
  â”‚     â””â”€ Similar per-tool statistics
  â””â”€ ExportMetrics(format: json|prometheus)

Thread-Safe Access:
  â€¢ sync.RWMutex protects SystemMetrics
  â€¢ Concurrent reads allow many goroutines
  â€¢ Exclusive writes during recording
```

---

### 2.4 Configuration & Validation (config.go, validation.go)

#### 2.4.1 Configuration Loading

**YAML Structure**:
```yaml
# crew.yaml
version: "1.0"
agents: ["orchestrator", "clarifier", "executor"]
settings:
  max_handoffs: 10
  max_rounds: 20
  timeout_seconds: 300

routing:
  signals:
    orchestrator:
      - signal: "[CLARIFY]"
        target: clarifier
  agent_behaviors:
    clarifier:
      wait_for_signal: true  # Pause and wait
      is_terminal: false
  parallel_groups:
    search_team:
      agents: ["faq_searcher", "knowledge_searcher"]
      next_agent: aggregator

# agents/agent_id.yaml
id: orchestrator
name: "Orchestrator"
role: "Request Router"
backstory: "You are..."
model: "gpt-4o-mini"
temperature: 0.7
is_terminal: false
tools: ["tool1", "tool2"]
handoff_targets: ["executor", "creator"]
system_prompt: |
  Custom system prompt with {{name}}, {{role}} variables
```

#### 2.4.2 Validation Framework (Issue #6, #16)

**Comprehensive Validation** (validation.go):
```
ConfigValidator:
  â”œâ”€ [Stage 1] validateCrewStructure()
  â”‚  â”œâ”€ Check crew config not nil
  â”‚  â”œâ”€ Check agents list not empty
  â”‚  â””â”€ Check settings reasonable
  â”‚
  â”œâ”€ [Stage 2] validateAgentReferences()
  â”‚  â”œâ”€ Check all referenced agents exist
  â”‚  â”œâ”€ Check tool references valid
  â”‚  â””â”€ Check handoff targets valid
  â”‚
  â”œâ”€ [Stage 3] validateRoutingConfiguration()
  â”‚  â”œâ”€ Check routing signals target valid agents
  â”‚  â”œâ”€ Check parallel groups configured correctly
  â”‚  â””â”€ Check no undefined agent behaviors
  â”‚
  â”œâ”€ [Stage 4] validateCircularRouting() â­ ADVANCED
  â”‚  â”œâ”€ Build routing graph
  â”‚  â”œâ”€ DFS cycle detection
  â”‚  â””â”€ Detect unreachable agents
  â”‚
  â””â”€ [Stage 5] reportValidationResults()
     â”œâ”€ Categorize errors vs warnings
     â””â”€ Provide actionable fix suggestions

Error vs Warning:
  â€¢ ERROR: Configuration won't work (crash/hang)
  â€¢ WARNING: Potential issues but workable
```

**Circular Routing Detection**:
```
Example Scenario:
  Agent A â†’ signal triggers â†’ Agent B
  Agent B â†’ signal triggers â†’ Agent C
  Agent C â†’ signal triggers â†’ Agent A  âš ï¸ CYCLE!

Detection Algorithm:
  1. Build adjacency list from routing config
  2. For each agent:
     â”œâ”€ Perform DFS from that agent
     â”œâ”€ Track visited nodes in current path
     â””â”€ If revisit node in same path â†’ CYCLE DETECTED
  3. Report all cycles with agents involved
```

---

### 2.5 HTTP Server & Streaming (http.go, streaming.go)

#### 2.5.1 HTTP Handler Architecture (Issue #1 - Race Condition)

**Race Condition Problem**:
```
BEFORE FIX:
  StreamHandler() reads executor.Verbose directly
  SetVerbose() modifies executor.Verbose
  âš ï¸ RACE: Multiple goroutines reading/writing same field

AFTER FIX (RWMutex):
  â€¢ RWMutex for read-heavy pattern
    (many StreamHandlers reading, few SetVerbose writing)
  â€¢ executorSnapshot: safe copy of state
  â€¢ Request-scoped executor: isolated per request
```

**Thread Safety Pattern**:
```go
type HTTPHandler struct {
  executor  *CrewExecutor
  mu        sync.RWMutex  // Protects executor field access
  validator *InputValidator
}

StreamHandler():
  1. h.mu.RLock()  // Multiple handlers can read concurrently
  2. snapshot := executorSnapshot{...}
  3. h.mu.RUnlock()
  4. Create request-scoped executor (isolated copy)
  5. Execute with request context

SetVerbose():
  1. h.mu.Lock()  // Exclusive access
  2. h.executor.Verbose = verbose
  3. h.mu.Unlock()
```

#### 2.5.2 Input Validation (Issue #10)

**InputValidator** (Lines 24-114):
```
ValidateQuery():
  â”œâ”€ Length check: 1-10,000 characters
  â”œâ”€ UTF-8 validation
  â”œâ”€ Null byte rejection
  â””â”€ Control character filtering (except \n, \t)

ValidateHistory():
  â”œâ”€ Max 1,000 messages
  â”œâ”€ Per-message validation:
  â”‚  â”œâ”€ Role must be in {"user", "assistant", "system"}
  â”‚  â”œâ”€ Max 100KB per message
  â”‚  â””â”€ Valid UTF-8
  â””â”€ Type-safe message structure

ValidateAgentID():
  â”œâ”€ Not empty
  â”œâ”€ Pattern: [a-zA-Z0-9_-]{1-128}
  â””â”€ Safe for routing decisions

SECURITY IMPLICATIONS:
  âœ… Prevents buffer overflow attacks
  âœ… UTF-8 validation prevents encoding exploits
  âœ… Control character filtering prevents injection
  âœ… Size limits prevent DoS via memory exhaustion
```

#### 2.5.3 Streaming Protocol (SSE - Server-Sent Events)

**Event Types**:
```
start          - Execution starting
agent_start    - Agent begins execution
agent_response - Agent returned response
tool_start     - Tool execution beginning
tool_result    - Tool result available
pause          - Waiting for signal (resume_agent_id in format)
error          - Error occurred
warning        - Partial failure
ping           - Keep-alive
done           - Execution completed

Format:
  data: {"type": "...", "agent": "...", "content": "...", "timestamp": "..."}
  (newline)(newline)
```

**Protocol Flow** (Lines 253-283):
```
while streamChan not closed:
  â”œâ”€ Select:
  â”‚  â”œâ”€ case event from streamChan
  â”‚  â”‚  â””â”€ Send event to client (SSE format)
  â”‚  â”‚  â””â”€ Flush response writer
  â”‚  â”œâ”€ case timeout 30s
  â”‚  â”‚  â””â”€ Send ping (keep-alive)
  â”‚  â””â”€ case context cancelled
  â”‚     â””â”€ Client disconnected, return
  â””â”€ On channel close:
     â”œâ”€ Check execErr (synced by channel close)
     â”œâ”€ Send final event (done or error)
     â””â”€ Close connection
```

---

### 2.6 Request Tracking (request_tracking.go)

**Purpose**: Correlate logs and events across distributed execution

```go
RequestMetadata {
  ID          string        // Unique UUID
  ShortID     string        // req-{first 12 chars}
  UserInput   string        // Original query
  StartTime   time.Time
  EndTime     time.Time
  Duration    time.Duration
  AgentCalls  int           // Number of agent executions
  ToolCalls   int           // Number of tool executions
  RoundCount  int           // Execution rounds
  Events      []Event       // All events in sequence
}

Event {
  Type        string        // agent_thinking, tool_call, etc.
  Agent       string        // Triggering agent
  Tool        string        // Tool name (if applicable)
  Timestamp   time.Time
  Data        interface{}   // Event-specific data
}

Usage Pattern:
  1. GenerateRequestID() â†’ create unique ID
  2. GetOrCreateRequestID(ctx) â†’ embed in context
  3. GetRequestID(ctx) â†’ retrieve in any goroutine
  4. All logs include request ID for correlation
```

---

### 2.7 Graceful Shutdown (shutdown.go)

**GracefulShutdownManager**:
```
Purpose: Ensure clean shutdown without losing active requests

State Management:
  â€¢ activeRequests: atomic counter
  â€¢ activeStreams: map[string]CancelFunc
  â€¢ isShuttingDown: atomic flag
  â€¢ GracefulTimeout: 30s default

Shutdown Sequence:
  1. Receive SIGTERM/SIGINT
  2. Set isShuttingDown flag
  3. Cancel all active streams
  4. Wait for requests to complete (with timeout)
  5. Call custom ShutdownCallback
  6. Exit
```

---

## 3. ğŸ”„ LUá»’NG THá»°C THI CHI TIáº¾T

### 3.1 Ká»‹ch Báº£n ÄÆ¡n Giáº£n: Single Agent, No Routing

```
User Request: "Kiá»ƒm tra bá»™ nhá»›"
â”‚
â”œâ”€ [1] HTTP Handler receives request
â”œâ”€ [2] Validate query (UTF-8, length, etc.)
â”œâ”€ [3] Create request-scoped executor
â”œâ”€ [4] Execute entry agent (Orchestrator)
â”‚  â””â”€ LLM returns response + tool calls
â”œâ”€ [5] Execute tools (with timeout tracking)
â”‚  â”œâ”€ Get system memory info
â”‚  â””â”€ Return results
â”œâ”€ [6] Check if Terminal â†’ YES
â”œâ”€ [7] Return response to client
â”‚
â””â”€ Client receives: "CPU: 45%, Memory: 2.5GB/8GB"
```

### 3.2 Ká»‹ch Báº£n Phá»©c Táº¡p: Multi-Agent Routing with Pause

```
User Request: "MÃ¡y tÃ­nh cá»§a tÃ´i cháº­m quÃ¡"
â”‚
â”œâ”€ [1] Orchestrator analyzes â†’ recognizes VAGUE
â”‚
â”œâ”€ [2] Orchestrator emits signal [CLARIFY]
â”‚
â”œâ”€ [3] Route to Clarifier (based on routing config)
â”‚
â”œâ”€ [4] Clarifier asks clarifying questions â†’ emits [Káº¾T THÃšC]
â”‚
â”œâ”€ [5] Clarifier has WaitForSignal=true â†’ PAUSE
â”‚  â””â”€ Send PAUSE event to client with agent ID
â”‚
â”œâ”€ [6] Client receives: event type=pause, content=[PAUSE:clarifier]
â”‚
â”œâ”€ User provides additional info
â”‚
â”œâ”€ [7] Client sends resume request with paused agent ID
â”‚
â”œâ”€ [8] Executor reads paused agent from request
â”‚  â””â”€ Route to Executor agent
â”‚
â”œâ”€ [9] Executor runs diagnostic tools (parallel group)
â”‚  â”œâ”€ GetCPUUsage (parallel)
â”‚  â”œâ”€ GetMemoryUsage (parallel)
â”‚  â”œâ”€ GetDiskSpace (parallel)
â”‚  â””â”€ Wait for all results
â”‚
â”œâ”€ [10] Executor is Terminal â†’ return results
â”‚
â””â”€ Client receives: Full diagnostics + recommendations
```

### 3.3 Error Recovery Flow

```
Tool Execution Error
â”‚
â”œâ”€ [1] safeExecuteToolOnce() catches error
â”‚
â”œâ”€ [2] classifyError() determines type
â”‚  â”œâ”€ Network error? â†’ Transient
â”‚  â”œâ”€ Timeout? â†’ Transient
â”‚  â””â”€ Validation? â†’ Non-transient
â”‚
â”œâ”€ [3] If non-transient â†’ return error immediately
â”‚
â”œâ”€ [4] If transient â†’ retry with backoff
â”‚  â”œâ”€ Attempt 1: Execute
â”‚  â”œâ”€ Attempt 2: Wait 100ms, Execute
â”‚  â”œâ”€ Attempt 3: Wait 200ms, Execute
â”‚  â””â”€ Max 2 retries = 3 total attempts
â”‚
â”œâ”€ [5] Record metrics (duration, status, error)
â”‚
â””â”€ [6] Return error or success result to agent
```

---

## 4. âš ï¸ CRITICAL ARCHITECTURAL DECISIONS

### 4.1 Thread Safety Strategy

| Component | Pattern | Rationale |
|-----------|---------|-----------|
| HTTPHandler | RWMutex (read-heavy) | Many concurrent reads, few writes |
| MetricsCollector | RWMutex + CAS | Protect metrics, atomic counters for requests |
| RequestMetadata | RWMutex | Shared across goroutines |
| Crew Executor | Isolated per-request copy | No shared state between requests |
| Tool execution | Context-based | Goroutine cancellation via context |

### 4.2 Timeout Strategy (Three-Layer)

```
Layer 1: Request Context (from HTTP server)
  â””â”€ Dies if client disconnects

Layer 2: Sequence Timeout (ToolTimeoutConfig.SequenceTimeout)
  â”œâ”€ Default: 30 seconds total for all tools
  â”œâ”€ Covers: tool1 + tool2 + tool3 + ...
  â””â”€ Prevents: one request consuming all resources

Layer 3: Per-Tool Timeout (ToolTimeoutConfig.PerToolTimeout)
  â”œâ”€ Default: 5 seconds each
  â”œâ”€ Adjusted: min(perToolTimeout, remainingSequenceTime - overhead)
  â””â”€ Prevents: one tool blocking all others
```

### 4.3 Error Recovery Strategy

```
Tool Execution Error
â”œâ”€ Classify: Transient or Permanent?
â”œâ”€ Transient: Retry up to 2 times with exponential backoff
â”œâ”€ Permanent: Fail immediately with clear error
â””â”€ Agent analyzes: Tool result or error message
   â””â”€ Agent decides: Proceed, retry different params, or fail?
```

**Why This Design**:
- âœ… Automatic recovery for flaky networks
- âœ… Fast failure for invalid configurations
- âœ… Agent intelligence can retry with different parameters
- âœ… Limited retries prevent infinite loops

### 4.4 Streaming Architecture Rationale

**Why SSE over WebSocket?**
- Simpler: No handshake, no bidirectional complexity
- Resilient: Auto-reconnect via EventSource API
- HTTP-friendly: Works through proxies, CDNs
- Unidirectional: Server â†’ Client (perfect for our use case)

**Why Channel-based Synchronization?**
```
PROBLEM (Issue #8):
  goroutine (ExecuteStream) writes to streamChan
  main goroutine (HTTP handler) reads from streamChan
  Both must synchronize access, handle race conditions

SOLUTION (Line 237):
  go func() {
    defer close(streamChan)  // Signal completion by closing
    execErr = executor.ExecuteStream(ctx, input, streamChan)
  }()

  for event := range streamChan {  // Automatically handles close
    ...
  }

  // Channel close provides:
  // 1. Happens-before: close() â†’ channel read returns
  // 2. Atomicity: Closing channel is atomic operation
  // 3. Idiomatic: Standard Go pattern for goroutine completion
```

---

## 5. ğŸ¯ DESIGN PATTERNS USED

| Pattern | Implementation | Purpose |
|---------|---|---------|
| **Factory** | NewCrewExecutor(), CreateAgentFromConfig() | Create complex objects safely |
| **Strategy** | RoutingConfig, AgentBehavior | Different execution strategies per config |
| **Observer** | StreamEvent, MetricsCollector | Monitor system state changes |
| **Timeout** | context.WithTimeout, TimeoutTracker | Handle long-running operations |
| **Retry** | retryWithBackoff, exponential backoff | Recover from transient failures |
| **Circuit Breaker** | error classification, retry limits | Prevent cascading failures |
| **Snapshot** | executorSnapshot | Safely capture state for readers |
| **Pipeline** | ExecuteStream loop with handoff | Sequential processing with routing |

---

## 6. ğŸ“Š PERFORMANCE CHARACTERISTICS

### 6.1 Scalability Analysis

| Metric | Characteristics |
|--------|---|
| **Concurrent Requests** | Bounded by active stream goroutines (one per request) |
| **Agent Concurrency** | Limited to MaxHandoffs (default: 5, but parallel groups allow many) |
| **Tool Concurrency** | Within agent: sequential, or parallel via parallel groups |
| **Memory Usage** | Per-request: ~10-50KB for state + history, unbounded for large context |
| **Latency** | Single tool: 1-5s, Agent execution: 1-2s, Full flow: 10-60s |

### 6.2 Resource Constraints

```
Timeout Dimensions:
  â€¢ Per-tool: 5s (configurable)
  â€¢ Sequence: 30s (configurable)
  â€¢ Request context: From HTTP client
  â€¢ Total: min(sequenceTimeout, perToolTimeout) Ã— MaxHandoffs

Memory:
  â€¢ OpenAI client cache: ~1MB per unique API key
  â€¢ Request state: ~50KB + message history
  â€¢ MetricsCollector: ~1MB for 1000s of executions

CPU:
  â€¢ YAML parsing: O(config size)
  â€¢ Routing lookup: O(agents) per handoff
  â€¢ Circular routing detection: O(agentsÂ²) at startup
```

---

## 7. ğŸ” SECURITY ARCHITECTURE

### 7.1 Input Validation Defense

```
Layer 1: HTTP Level
  â€¢ Method validation (GET/POST)
  â€¢ Content-Type checking
  â€¢ Header sanitization

Layer 2: Application Level
  â”œâ”€ Query validation
  â”‚  â”œâ”€ Length: 1-10,000 chars
  â”‚  â”œâ”€ UTF-8 validation
  â”‚  â”œâ”€ Null byte rejection
  â”‚  â””â”€ Control character filtering
  â”œâ”€ History validation
  â”‚  â”œâ”€ Max 1000 messages
  â”‚  â”œâ”€ Role whitelist: {user, assistant, system}
  â”‚  â””â”€ Per-message size limit: 100KB
  â””â”€ AgentID validation
     â”œâ”€ Pattern: [a-zA-Z0-9_-]{1-128}
     â””â”€ Prevents directory traversal, injection

Layer 3: Execution Level
  â€¢ Tool argument validation
  â€¢ Parameter type checking
  â€¢ Tool existence verification
```

### 7.2 Threat Model & Mitigations

| Threat | Mitigation |
|--------|---|
| **DoS via large input** | Max query/history size limits, configurable |
| **Memory exhaustion** | Client cache TTL (1h), tool output truncation (2000 chars) |
| **Long-running tasks** | Timeout at sequence level (30s), per-tool (5s) |
| **Goroutine leaks** | Using errgroup for parallel execution, proper cleanup |
| **Concurrent access bugs** | RWMutex for shared state, isolated per-request copies |
| **Tool misuse** | Argument validation, tool existence check |
| **Infinite loops** | MaxHandoffs limit (default 5), max rounds |

---

## 8. ğŸ§ª TEST COVERAGE

**Test Files** (7 files, ~1,500 lines):
- `agent_test.go` - Agent execution, tool call extraction
- `crew_test.go` - Crew coordination, routing, parallel execution
- `config_test.go` - Config loading and validation
- `http_test.go` - HTTP handler, streaming
- `validation_test.go` - Input validation
- `request_tracking_test.go` - Request metadata tracking
- `shutdown_test.go` - Graceful shutdown

**Coverage Areas**:
- âœ… Happy path: Single agent, tool execution, response
- âœ… Error handling: API failures, validation errors, timeouts
- âœ… Concurrency: Race condition detection, parallel execution
- âœ… Configuration: Valid/invalid configs, circular routing
- âœ… Routing: Signal matching, handoff logic, parallel groups

---

## 9. ğŸš€ PRODUCTION READINESS CHECKLIST

| Category | Status | Evidence |
|----------|--------|----------|
| **Error Recovery** | âœ… | Panic recovery, retry logic, error classification |
| **Monitoring** | âœ… | Metrics collection, request tracking, logging |
| **Concurrency** | âœ… | RWMutex, context propagation, goroutine cleanup |
| **Timeout Safety** | âœ… | Three-layer timeout strategy |
| **Resource Limits** | âœ… | Input validation, memory limits, timeout bounds |
| **Graceful Shutdown** | âœ… | Signal handling, active request tracking |
| **Configuration** | âœ… | Validation, circular routing detection |
| **Testing** | âœ… | 7 test files with multi-scenario coverage |

---

## 10. ğŸ“ˆ METRICS & OBSERVABILITY

### Key Metrics Exposed

```
Per-Agent:
  â€¢ ExecutionCount: Total agent executions
  â€¢ SuccessCount: Successful completions
  â€¢ ErrorCount: Failures
  â€¢ AverageDuration: Performance baseline
  â€¢ MinDuration / MaxDuration: Performance range

Per-Tool:
  â€¢ ExecutionCount: Total tool calls
  â€¢ SuccessCount: Successful executions
  â€¢ ErrorCount: Failed calls
  â€¢ AverageDuration: Performance
  â€¢ Timeout tracking: Deadline exceeded counts

System-wide:
  â€¢ TotalRequests: Request volume
  â€¢ SuccessRate: Success percentage
  â€¢ AverageRequestTime: Latency
  â€¢ MemoryUsage: Current and peak
  â€¢ CacheHitRate: Client cache effectiveness

Export Formats:
  â€¢ JSON: Structured data for dashboards
  â€¢ Prometheus: Time-series metrics for monitoring
```

---

## 11. ğŸ“ KEY ARCHITECTURAL INSIGHTS

### 11.1 Agent Independence

Each agent is **stateless** with respect to system state:
- Agent decisions depend on: role, backstory, tools, conversation history
- Agent output: response + tool calls (completely deterministic for same input)
- **Implication**: Easy to test, compose, and parallelize

### 11.2 Configuration-Driven Routing

Instead of hard-coded agent flow:
```yaml
# Configuration defines routing
signals:
  orchestrator:
    - signal: "[CLARIFY]"
      target: clarifier
```

Instead of:
```go
// Hard-coded logic
if response.Contains("[CLARIFY]") {
  nextAgent = clarifier
}
```

**Benefits**:
- âœ… Routing changes without code deployment
- âœ… New agents without touching orchestration
- âœ… Easy to test: validate config before runtime

### 11.3 Tool Call Extraction Robustness

**Hybrid approach** (native + fallback):
- Modern models: Use OpenAI's structured tool_calls (validated by OpenAI)
- Legacy/edge cases: Parse text response with pattern matching
- **Rationale**: Balance between correctness and flexibility

### 11.4 Per-Request Isolation

```go
// Shared state (read-only after creation)
handler.executor.crew      // All requests share crew definition
handler.executor.apiKey    // All requests share API key

// Per-request state (isolated copy)
executor.history           // Each request has own copy
executor.Verbose           // Snapshot per request
executor.ResumeAgentID     // Isolated per request
```

**Benefits**:
- âœ… No cross-request interference
- âœ… Concurrent requests don't block each other
- âœ… Safe to pause/resume individual requests

---

## 12. ğŸ”® ARCHITECTURAL EVOLUTION POTENTIAL

### Current Limitations & Solutions

| Limitation | Current Behavior | Potential Enhancement |
|-----------|---|---|
| **Parallel groups** | All agents run, aggregate results | Smart filtering: run only relevant agents |
| **Tool parallelization** | Sequential within agent | Allow per-tool parallelization |
| **Context reuse** | Per-request copy | Streaming context updates |
| **Monitoring depth** | Per-agent metrics | Per-agent-per-round metrics |
| **Configuration** | YAML files | Dynamic configuration API |
| **Tool discovery** | Static definition | Dynamic tool registration |

---

## 13. ğŸ“ CONCLUSION

### Core Strengths

1. **Architectural Clarity**: Clear separation of concerns across layers
2. **Error Resilience**: Multi-layer error recovery and classification
3. **Performance Safety**: Three-layer timeout strategy prevents resource exhaustion
4. **Observability**: Comprehensive metrics and request tracking
5. **Concurrency Safety**: Proper synchronization with minimal locks
6. **Configuration Driven**: Routing and behavior externalized to YAML
7. **Production Ready**: Graceful shutdown, input validation, comprehensive testing

### Complexity Trade-offs

- âœ… Added complexity: Circular routing detection, timeout tracking, parallel execution
- âœ… Justified by: Production safety, debugging capability, scaling potential
- âœ… Managed by: Comprehensive documentation, test coverage, validation framework

### Recommended Usage Pattern

```
1. Define crew.yaml with agent list and routing
2. Define agents/{id}.yaml with agent configuration
3. Load configuration with validation:
   executor, err := NewCrewExecutorFromConfig(apiKey, configDir, tools)
4. Start HTTP server:
   StartHTTPServer(executor, 8080)
5. Client submits requests via SSE:
   GET /api/crew/stream?q="user query"
6. Monitor metrics:
   GET /health
   GET /metrics?format=prometheus
```

---

**Total LOC**: 9,436 lines
**Files**: 20 (13 main + 7 test)
**Test Coverage**: Unit + Integration
**Production Status**: âœ… Ready for deployment

Last analyzed: 2025-12-22
